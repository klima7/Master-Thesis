\section{RESDSQL}
\code{RESDSQL} to dość nowe rozwiązanie, gdyż powstało na początku 2023 roku. Zostało opublikowane w artykule \bibtitle{RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL} \mycite{Li2023resdsql} przez Haoyang Li i innych. Na chwilę pisania niniejszej pracy jest to najwyżej znajdujące się w rankingu \code{Spider} rozwiązanie, które nie opiera się na wykorzystaniu dużych pretrenowanych modeli językowych od \code{OpenAI} i do którego dostępny jest kod źródłowy \mycite{resdsql-repository}.

\subsection{Działanie}
\code{RESDSQL} bazuje w dużej mierzę na pretrenownym modelu językowym \code{T5} \mycite{Raffel2019}. Jest to kompletny model typu transformer, który służy do generowania tekstowych odpowiedzi na podstawie tekstowych informacji wejściowych i został już wcześniej wytrenowany na obszernym zbiorze, więc zawiera dużą ilość wiedzy. Występuje  \code{RESDSQL} dokonuje dotrenowania tego modelu tak, aby zwracał zapytania SQL będące odpowiedzią na podawane pytania. Podczas konstruowania sekwencji wejściowej oraz dekodowania sekwencji wyjściowej dodaje jednak ciekawe techniki, które poprawiają jego skuteczność dla rozważanego problemu \code{Text-to-SQL}. Ich głównym celem jest rozdzielenie od siebie generowania szkieletu zapytań i uzupełniania go konkretnymi nazwami tabel i kolumn, co z resztą zostało podkreślone w tytule artykułu.

Ważnym elementem, odróżniającym \code{RESDSQL} od wcześniej opisanych rozwiązań jest to, że do enkodera zawartego wewnątrz \code{T5} nie są przekazywane wszystkie nazwy tabel i kolumn, lecz tylko te, które zostały uznane w kontekście danego pytania za najbardziej istotne. Celem tego działania jest odciążenie enkodera z wykonywania skomplikowanego procesu \code{schema linking}. Aby dostać potrzebną informację o istotności poszczególnych tabel i kolumn dla podanego pytania trenowana jest wcześniej całkowicie niezależna sieć, która przyjmuję pytanie oraz wszystkie nazwy tabel i kolumn i dla każdej z nich zwraca prawdopodobieństwo, które może być interpretowane jako stopień istotności. Sieć ta określana jest przez swoich autorów mianem \code{cross-encoder}. Ostatecznie do modelu \code{T5} przekazywane są 4 najistotniejsze tabele, a dla każdej z nich 5 najważniejszych kolumn.

Z punktu widzenia dekodowania najważniejszą nowością jest to, że model \code{T5} nie jest dotrenowywany tak, aby od razu produkować kompletne zapytania, lecz na początku zwrócić jego szkielet, a dopiero za nim wykonywalne zapytanie. Uzasadnieniem opisanego zachowania jest to, że początkowe stworzenie szkieletu jest w miarę prostym problemem, więc powinno mieć dużą dokładność. Dekodery w modelach językowych posiadają własność nazywaną autoregresyjnością, która polega na tym, że podczas generowania kolejnych fragmentów odpowiedzi brany jest pod uwagę tekst wygenerowany do tej pory. Dzięki temu model \code{T5} podczas produkowania drugiej części odpowiedzi, która zawiera kompletne zapytanie SQL, może odwoływać się do wcześniejszej części z szablonem i traktując ją jako pewnego rodzaju notatnik uzupełnić brakujące elementy.

Ciekawą techniką z którą \code{RESDSQL} można łączyć jest \code{NatSQL} \mycite{Gan2021natsql}. Wprowadza ona alternatywną reprezentację dla zapytań SQL, która bardziej przypomina język naturalny i w związku z tym jest łatwiejsza do nauczenia i generowania przez większość modeli. Jednocześnie jest ona na tyle jednoznaczna, że można przekonwertować ją na tradycyjne zapytania SQL. Zgodnie z \mycite{Li2023resdsql} zastąpienie tradycyjnych zapytań za pomocą \code{NatSQL} pozwoliło na zbiorze \code{Spider} zwiększyć skuteczności mierzonej metryką \code{EM} o około 2 procent.

\subsection{Modyfikacje dla języka polskiego}
Rozwiązanie \code{RESDSQL} okazało się wyjątkowo proste do przystosowania dla języka polskiego, bo nie trzeba było wykonywać żadnych kreatywnych modyfikacji. Metoda ta została bowiem już wcześniej wykorzystana do pracy na rosyjskim zbiorze \code{PAUQ} i stosowny kod znajdywał się w repozytorium. Wystarczyło dodać kilka nowych skryptów, które nieznacznie różnią się od istniejących.

Najbardziej istotną modyfikacją, niezbędną do nauki na polskim, czy też rosyjskim tłumaczeniu jest zmiana wykorzystywanego modelu \code{T5} na \code{mT5} \mycite{Xue2020}. Różnica pomiędzy nimi jest jedynie taka, że pierwszy został nauczony na tekstach angielskich, natomiast drugi na zbiorze zawierającym 101 różnych języków, w tym wspomniane dwa.

Okazuję się, że niestety dla języka polskiego nie można łatwo wykorzystać obiecującego połączenia niniejszej metody z \code{NatSQL}. Przyczyną tego jest brak udostępnienia przez autorów \code{NatSQL} skryptu przekształcającego tradycyjne zapytania SQL do zaprojektowanej przez nich postaci. Opublikowane są jedynie zapytania ze zbioru \code{Spider} po dokonaniu takiej konwersji. W przypadku rosyjskiego tłumaczenia udało się zastosować \code{NatSQL}, lecz wymagało to przekształcenia każdego zapytania w sposób ręczny. Dla zbioru polskiego należałoby postąpić analogicznie, co wymagałoby wiele żmudnej pracy, której postanowiono uniknąć.

\subsection{Eksperymenty}
W ramach eksperymentu postanowiono dokonać nauki \code{RESDSQL} na zbiorze \code{mix-spider}, ponieważ jak wcześniej zauważono, nauka na dwujęzycznych zbiorach wydaje się mieć lepsze efekty niż na zbiorach jednojęzycznych. Konieczny był wybór konkretnego wariantu dotrenowywanego modelu \code{mT5}, ponieważ występuje on w kilku rozmiarach. Na mocniejszej z dwóch posiadanych kart graficznych, czyli \code{RTX 3080}, udało się uruchomić jedynie wariant najmniejszy -- \code{mt5-base}. Sprawdzając poszczególne rozmiary stosowano minimalną wielkość wsadu (ang. batch size), czyli ilość danych jednocześnie przekazywanych do modelu, aby zmniejszyć zapotrzebowanie na pamięć VRAM. Po dokonaniu głębszej ingerencji w procedurę treningu udało się ostatecznie uruchomić ją także dla większego modelu. Wymagało to jednak załadowania wag i treningu z wykorzystaniem arytmetyki 16-bitowej zamiast powszechnie stosowanej 32-bitowej. Może się to wiązać z pojawieniem podczas nauki różnych niestabilności, więc z powodu braku doświadczenia z tego typu treningiem postanowiono pozostać przy 

Zgodnie z wcześniejszym opisem wykorzystanie \code{RESDSQL} wymaga w pierwszej kolejności wytrenowania sieci \code{cross-encoder}, która dokonuje oceny istotności tabel i kolumn. Trening ten wykonywano przez 4 godziny, a wykres przedstawiający zmianę skuteczności modelu w jego trakcie na części testowej umieszczono na rysunku \ref{plot:resdsql-classifier-accuracy}. Wspomniana skuteczność jest tutaj wyrażana za pomocą metryki \code{AUC} \mycite{Ling2003}, popularnej dla zadań klasyfikacji. Po 4 godzinach trening został automatycznie przerwany, ponieważ od dłuższego czasu nie nastąpiło zwiększenie skuteczności rozumianej jako sumy \code{AUC} dla tabel i kolumn. Jako produkt tego etapu został wybrany model z zestawem wag, dla którego metryka ta była najwyższa.

\begin{figure}[ht!]
\centering
\begin{subfigure}{0.49\textwidth}
    \begin{tikzpicture}
      \begin{axis}[
        width=\linewidth,
        height=\linewidth,
        xmin=0, xmax=4.1,
        grid=major,
        xlabel={Czas treningu [godziny]},
        ylabel=AUC,
        legend cell align={left},
        legend pos=south east,
      ]
        \addplot table[x=time,y=table,col sep=comma] {plots/resdsql_classifier_training.csv}; 
        \addplot table[x=time,y=column,col sep=comma] {plots/resdsql_classifier_training.csv}; 
        \legend{dla tabel, dla kolumn}
      \end{axis}
    \end{tikzpicture}
    \caption{model oceniający istotność tabel i kolumn}
    \label{plot:resdsql-classifier-accuracy}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \begin{tikzpicture}
      \begin{axis}[
        width=\linewidth,
        height=\linewidth,
        xmin=0, xmax=14,
        grid=major,
        xlabel={Czas treningu [godziny]},
        ylabel=EM without values,
      ]
        \addplot table[x=step,y=em,col sep=comma] {plots/resdsql_t5_training.csv}; 
      \end{axis}
    \end{tikzpicture}
    \caption{model przewidujący zapytania}
    \label{plot:resdsql-t5-accuracy}
\end{subfigure}
\caption{Wykresy skuteczności modeli \code{RESDSQL} na części testowej zbioru \code{Pol-Spider} wraz z kolejnymi krokami treningu}
\label{plot:resdsql-accuracy}
\end{figure}

Drugim etapem nauki było dotrenowywanie modelu \code{T5} dla zadania generowania zapytań SQL. Tą część treningu zakończono po 14 godzinach. Tak jak jest to ukazane na rysunku \ref{plot:resdsql-t5-accuracy} przez pierwsze 7 godzin dokładność generowanych zapytań rosła, lecz potem zaczęła regularnie spadać i w godzinie 14 osiągnęła na tyle niską wartość, że dalszego treningu zaprzestano. Podejrzewa się, że przyczyną tego jest przetrenowanie modelu, czyli zbytnie przystosowanie do danych treningowych lub zjawisko katastroficznego zapominania (ang. catastrophic forgetting) \mycite{Kirkpatrick2016}. Pierwotny model \code{T5} mógł bowiem zawierać istotną wiedzę, która została podczas dotrenowywania nadpisana. Modele są bowiem dość ograniczone pod kątem sekwencyjnej nauki zadań. Jako finalny został wybrany model z punktu w którym wartość rozważanej metryki była najwyższa.

\subsection{Wyniki}
W tabeli \ref{tab:resdsql-difficulty} przedstawione zostały wyniki przeprowadzonego eksperymentu. Rozważany model, czyli \code{RESDSQL}, podobnie jak poprzedni, generuje kompletne zapytania SQL z wartościami. Możliwe więc było obliczenie dodatkowych metryk \code{EM with values} oraz \code{EX}, co też uczyniono.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|r|r|r|r|r|}
        \hline
        \thead{Zbiór} & \thead{Easy} & \thead{Medium} & \thead{Hard} & \thead{Extra} & \thead{Razem} \\
        \hline
        Pol-Spider & 
        \threevals{76,2}{70,6}{83,5} &
        \threevals{61,9}{56,3}{73,1} &
        \threevals{50,0}{46,0}{62,9} &
        \threevals{35,8}{30,4}{53,3} &
        \threevals{59,1}{53,8}{70,7} \\
        
        Pol-Spider-PL &
        \threevals{78,6}{72,6}{85,1} &
        \threevals{64,1}{58,5}{74,4} &
        \threevals{50,0}{44,8}{62,1} &
        \threevals{32,5}{27,1}{50,0} &
        \threevals{60,2}{54,5}{71,0} \\
        
        Pol-Spider-EN &
        \threevals{73,8}{68,5}{81,9} &
        \threevals{59,6}{54,0}{71,7} &
        \threevals{50,0}{47,1}{63,8} &
        \threevals{39,2}{33,7}{56,6} &
        \threevals{58,1}{53,1}{70,4} \\
        
        En-Spider &
        \threevals{81,5}{79,4}{86,3} &
        \threevals{69,3}{66,4}{75,8} &
        \threevals{51,7}{50,6}{65,5} &
        \threevals{47,0}{45,8}{50,0} &
        \threevals{65,7}{63,5}{72,4} \\
        
        \hline
        
        Pol-Spidersyn &
        \threevals{61,7}{57,6}{72,5} &
        \threevals{52,2}{48,7}{65,8} &
        \threevals{42,6}{41,9}{57,0} &
        \threevals{26,4}{21,9}{44,6} &
        \threevals{48,6}{45,3}{62,4} \\
        
        Pol-Spiderdk &
        \threevals{52,7}{48,2}{62,3} &
        \threevals{34,8}{31,5}{50,2} &
        \threevals{20,9}{20,9}{37,8} &
        \threevals{17,6}{13,3}{32,4} &
        \threevals{33,2}{29,9}{47,5} \\
        
        Pol-Sparc &
        \threevals{61,3}{59,0}{72,3} &
        \threevals{38,3}{33,0}{60,2} &
        \threevals{13,3}{13,3}{43,3} &
        \threevals{43,8}{43,8}{50,0} &
        \threevals{52,5}{49,5}{67,2} \\
        
        Pol-Cosql &
        \threevals{61,7}{56,7}{71,3} &
        \threevals{54,2}{48,3}{63,6} &
        \threevals{26,5}{25,0}{51,5} &
        \threevals{20,6}{20,6}{55,9} &
        \threevals{51,5}{47,2}{65,2} \\
        
        \hline
    \end{tabular}
    \caption{Wyniki modelu \code{RESDSQL} na poszczególnych zbiorach. Wartości w każdej komórce posiadają format EM \slashsep{ EM with values} \slashsep{ EX}}
    \label{tab:resdsql-difficulty}
\end{table}

\subsection{Analiza}
W porównaniu z dwoma wcześniej analizowanymi rozwiązaniami \code{RESDSQL} wydaje się wypadać najlepiej. Inaczej mówią jedynie wyniki dwóch testów. W przypadku zbiorów \code{Pol-Sparc} oraz \code{Pol-Spiderdk} patrząc na metrykę \code{EM} najlepiej wypadł bowiem model pierwszy, czyli \code{RAT-SQL}. W pozostałych przypadkach to jednak \code{RESDSQL} dominuje.

Warto zwrócić uwagę na fakt, że tak dobre wyniki udało się osiągnąć w stosunkowo niedługim czasie. Pomimo tego, że nauka odbywała się w sposób dwuetapowy to sumaryczny czas po którym modele osiągnęły maksymalną skuteczność okazał się dla \code{RESDSQL} najkrótszy spośród rozwiązań do tej pory rozważanych.

Tak jak zostało wcześniej zauważone, \code{RESDSQL} jest dość świeżym rozwiązaniem, które zostało opublikowane już po stworzeniu wszystkich istniejących tłumaczeń zbioru \code{Spider}, więc ich twórcy nie mieli nawet możliwości go przetestować. Autorzy metody \code{RESDSQL} kilka miesięcy od wydania swojego artykułu wyszli jednak z taką inicjatywą i postanowili dokonać treningu i testów swojego modelu na chińskim tłumaczeniu, czyli zbiorze \code{CSpider}. Dokonali tego dla wszystkich możliwych konfiguracji, czyli z wykorzystaniem modelu językowego \code{T5} we wszystkich rozmiarach oraz z użyciem techniki \code{NatSQL} i bez niej. W tabeli \ref{tab:resdsql-translations-results} zestawiono wyniki osiągnięte z wykorzystaniem modelu \code{RESDSQL} na polskim zbiorze oraz chińskim za pomocą modelu o odpowiadającej mu konfiguracji.

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|r|r|}
        \hline
        \multirow{2}{*}[-0.8em]{\thead{Tłumaczenie \\zbioru Spider}} & \multicolumn{2}{c|}{\thead{Zbiór treningowy}} \\
        \cline{2-3}
        \multirow{2}{*}{} & \thead{Tłumaczenie} & \thead{Tłumaczenie\\ + angielski} \\
        \hline
        \multicolumn{1}{|l|}{Rosyjskie (\code{PAUQ})} & \twovals{71,7}{77,9} & \twovals{---}{---} \\
        \multicolumn{1}{|l|}{Polskie (\code{Pol-Spider})} & \twovals{---}{---} & \twovals{59,1}{70,7} \\
        \hline
    \end{tabular}
    \caption{Zestawienie wyników osiągniętych przez model \code{RESDSQL} dla różnych tłumaczeń zbioru \code{Spider} i zbiorów treningowych}
    \label{tab:resdsql-translations-results}
\end{table}

 Zgodnie z zawartymi w tabeli \ref{tab:resdsql-translations-results} danymi liczbowymi na rosyjskim zbiorze udało się osiągnąć wartość metryki \code{EM} większą o ponad 10 p.p. niż dla zbioru polskiego. Do tego treningu dokonano wyłącznie na tekstach rosyjskich, bez połączenia ze zbiorem angielskim, co przypuszczalnie powinno dać jeszcze lepsze rezultaty. Duża różnica pomiędzy wynikami otrzymanymi na polskim i rosyjskim tłumaczeniu jest bardzo zastanawiająca i doszukano się dwóch możliwych jej przyczyn. Pierwsza to hipotetycznie niższy poziom skomplikowania języka rosyjskiego, co jednak jest trudnym do zweryfikowania. Drugim, zdecydowanie istotnym powodem jest fakt, że wykorzystywany w obu przypadkach wielojęzyczny model \code{mT5} był trenowany na zbiorze zawierającym większą ilość tekstów w języku rosyjskim niż polskim. Dokładny procentowy udział każdego języka w zbiorze treningowym można znaleźć w załączniku artykułu wprowadzającego model \code{mT5} \mycite{Xue2020}. Zgodnie z nim zbiór treningowy zawierał 3,71\% danych w języku rosyjskim i tylko 2,15\% w języku polskim.