\section{C3}
\code{C3} jest dość nowym rozwiązaniem, które zostało opublikowane w lipcu 2023 roku w artykule pod tytułem \bibtitle{C3: Zero-shot Text-to-SQL with ChatGPT} \mycite{Dong2023}. Należy ono do niedawno powstałego nurtu zaprzęgającego duże modele językowe do generowania zapytań SQL. Zapytania są kompletne, gdyż model ten przewiduje również odpowiednie wartości. Cały kod źródłowy \code{C3} został udostępniony przez autorów za pośrednictwem platformy GitHub \mycite{c3-repository}. 

\subsection{Działanie}
Sposób działania \code{C3} istotnie odbiega od funkcjonowania opisywanych wcześniej rozwiązań, gdyż należy ono do zupełnie innej kategorii. Zgodnie z wprowadzonym we wcześniejszym rozdziale podziałem nie jest to model dedykowany, lecz rozwiązanie wykorzystujące duże modele językowe wraz z techniką \code{prompt engineering}. Oznacza to, że żaden trening nie jest potrzebny, a nacisk przeniesiony został na skonstruowanie wejścia do modelu \code{GPT-3.5-Turbo}, które pozwoli jak najlepiej aktywować i wykorzystać zawartą w nim wiedzę. Dane wejściowe do tego modelu mają postać konwersacji, czyli naprzemiennie występujących wiadomości od człowieka oraz od inteligentnego asystenta. Wyjściem jest natomiast wiadomość od asystenta, będąca kontynuacją ten konwersacji.

W celu przewidzenia kompletnego zapytania SQL model \code{GPT-3.5-Turbo} jest wykorzystywany trzykrotnie. W pierwszej kolejności przekazywane jest mu pytanie wraz ze schematem bazy danych i proszony jest o zwrócenie nazw tabel posortowanych względem istotności. W drugim kroku w instrukcji wejściowej przekazywane jest pytanie, cztery najistotniejsze tabele wraz z kolumnami oraz relacje między nimi i model proszony jest o posortowanie kolumn w obrębie każdej tabeli od najbardziej do najmniej istotnej. Ostatecznie do \code{GPT-3.5-Turbo} przekazywane jest pytanie, elementy schematu uznane za istotne, klucze obce oraz wartości z bazy danych znalezione w procesie \code{schema linking} i jest on proszony o wygenerowanie gotowego zapytania.

Podczas tworzenia \code{C3} wykorzystano 3 istotne techniki od których model wziął swoją nazwę. Określono jest bowiem jako \code{\underline{C}lear Prompting}, \code{\underline{C}alibration with Hints} oraz \code{\underline{C}onsistent Output}. Pierwsza z nich polega na tworzeniu instrukcji wejściowych o przejrzystym układzie i zamieszczaniu w nich jedynie elementów najważniejszych. Druga sprowadza się do obserwacji odchyleń względem oczekiwań w odpowiedziach modelu językowego i podawaniu we wcześniejszej konwersacji wskazówek, które to korygują. Ostatnia technika oznacza intensywne wykorzystanie strategii nazywanej \code{self-consistency} \mycite{Wang2022Consistency}. Jest ona związana z faktem, że odpowiedzi modeli językowych nie są deterministyczne - przekazywanie tych samych informacji wejściowych zwykle skutkuje różnymi odpowiedziami. W związku z tym często dobrym pomysłem jest podanie do modelu tych samych informacji kilka razy, zebranie wszystkich odpowiedzi i wyłonienie spośród nich tej najczęstszej.

\subsection{Modyfikacje dla języka polskiego}
Z punktu widzenia przystosowania \code{C3} do języka polskiego najistotniejsza była modyfikacja promptów, czyli instrukcji wejściowych do modelu językowego. Poza tym, tak jak w przypadku \code{RAT-SQL}, należało zamienić wyrazy \code{stop words} z angielskich na polskie. Są one wykorzystywane na etapie \code{schema linking} podczas poszukiwania połączeń \code{value link}.

Modyfikacji promptów można było dokonać tego na różne sposoby i rozważono kilka z nich. Pierwszą możliwością jest przetłumaczenie ich w całości na język polski. Wymaga to jednak całkowitej modyfikacji i w związku z tym wysiłek twórców \code{C3} włożony w ich dopracowanie jest w dużym stopniu tracony. Poza tym modele językowe są uczone w większości na danych angielskich i to dla nich osiągają najlepsze wyniki. Drugim z rozważanych podejść był kompletny brak tłumaczenia promptów. Nagłe pojawienie się polskiego pytania oraz elementów schematu w instrukcji wejściowej wydaje się jednak być dość nietypowe i niespodziewane, co przypuszczalnie może pogarszać zwracane wyniki. Z wymienionych powodów zdecydowano się pozostawić prompty w języku angielskim, lecz wprowadzić w nich kilka delikatnych modyfikacji. W instrukcjach służących do znalezienia istotnych tabel i kolumn słowo \textit{question} określono przymiotnikiem \textit{polish}. W instrukcji służącej do finalnego generowania zapytania zrobiono to samo ze słowem \textit{databases}, przetłumaczono dwa przykładowe pytania na język polski oraz dodano zdanie mówiące, że pytanie zostanie dostarczone w języku polskim. Zmodyfikowane prompty z zaznaczonymi zmianami można znaleźć w dodatku \ref{extra:c3-prompts}. Bez wątpienia ciekawym byłoby eksperymentalne zbadanie skuteczności każdego z przedstawionych sposobów modyfikacji instrukcji, lecz byłoby to również kosztowne.

\subsection{Eksperymenty}
Zmodyfikowane w powyżej opisany sposób rozwiązanie \code{C3} chciano przetestować na tych samych zbiorach co wszystkie wcześniejsze. Problem stanowią w tym przypadku jednak koszty naliczane przez \code{OpenAI} za wykorzystanie modelu \code{GPT-3.5-Turbo}, które są znaczące. Aby temu sprostać postanowiono skonstruować mniejsze odpowiedniki potrzebnych zbiorów. Poszczególne próbki wybrano przy tym tak, aby pod względem rozkładu poziomów trudności zapytań SQL zbiory mniejsze jak najdokładniej odpowiadały oryginalnym. W przypadku zbiorów zawierających elementy schematu zarówno w języku polskim i angielskim zadbano także o to, aby proporcje pomiędzy tymi częściami nie uległy zmianie. Zbiory ze schematem w jednym języku postanowiono zredukować do 50 przykładów, natomiast z dwujęzycznymi schematami do 100 przykładów. 

Wykonanie ewaluacji na wybranych pięciuset przykładach spowodowało naliczenie kwoty około 10 dolarów, czyli w przybliżeniu 40 złotych. W przeliczeniu na jedno pytanie daje to około 8 groszy. Względnie wysokie koszty są w dużym stopniu spowodowane wykorzystaniem wspomnianego mechanizmu \code{self-consistency}, który wymaga odpytywania modelu językowego wielokrotnie o to samo. 

\subsection{Wyniki}
Wyniki wykonanej ewaluacji przedstawiono w tabeli \ref{tab:c3sql-difficulty}. Mają one format podobny do wcześniejszych, lecz ze względu na małą liczność zbiorów podawanie części ułamkowej poszczególnych metryk było bezzasadne. W nazwach zbiorów zawarto liczbę znajdujących się w nich przykładów.

\begin{table}[H]
    \centering


    \begin{tabular}{|l|r|r|r|r|r|}
        \hline
        \thead{Zbiór} & \thead{Easy} & \thead{Medium} & \thead{Hard} & \thead{Extra} & \thead{Razem} \\
        \hline
        pol-spider-100 & 
        \threevals{54}{54}{83} &
        \threevals{43}{43}{91} &
        \threevals{44}{38}{44} &
        \threevals{13}{13}{75} &
        \threevals{41}{40}{79} \\
        
        pol-spider-pl-50 &
        \threevals{58}{58}{83} &
        \threevals{50}{50}{91} &
        \threevals{38}{38}{63} &
        \threevals{13}{13}{75} &
        \threevals{44}{44}{82} \\
        
        pol-spider-en-50 &
        \threevals{50}{50}{83} &
        \threevals{36}{36}{91} &
        \threevals{50}{38}{25} &
        \threevals{13}{13}{75} &
        \threevals{38}{36}{76} \\
        
        \hline
        
        pol-spidersyn-100 &
        \threevals{32}{32}{55} &
        \threevals{39}{36}{61} &
        \threevals{28}{27}{56} &
        \threevals{0}{0}{38} &
        \threevals{29}{28}{55} \\
        
        pol-spiderdk-100 &
        \threevals{100}{75}{75} &
        \threevals{50}{48}{59} &
        \threevals{36}{36}{79} &
        \threevals{15}{15}{50} &
        \threevals{51}{45}{63} \\
        
        pol-sparc-100 &
        \threevals{61}{61}{79} &
        \threevals{22}{18}{68} &
        \threevals{0}{0}{50} &
        \threevals{0}{0}{0} &
        \threevals{46}{45}{73} \\
        
        pol-cosql-100 &
        \threevals{62}{56}{87} &
        \threevals{31}{31}{77} &
        \threevals{29}{22}{36} &
        \threevals{0}{0}{50} &
        \threevals{44}{40}{74} \\
        
        \hline
    \end{tabular}
    \caption{Wyniki modelu \code{C3} na poszczególnych zbiorach. Wartości w każdej komórce posiadają format EM \slashsep{ EM with values} \slashsep{ EX}}
\label{tab:c3sql-difficulty}
    
    
\end{table}

\subsection{Analiza}
Zgodnie z publicznym rankingiem zbioru \code{Spider} analizowane teraz rozwiązanie jest spośród wszystkich w tej pracy omawianych najlepsze pod kątem metryki EX. Przeprowadzone eksperymenty to potwierdzają. Najlepsze wyniki nie zostały osiągnięte jedynie w przypadku zbioru \code{pol-spidersyn}, lecz również są dość wysokie.

Wartość metryki \code{EM} osiągnęła z drugiej strony zaskakująco niską wartość. Pod jej kątem \code{C3} wydaje się być najgorszym z analizowanych rozwiązań. Skąd wynika tak duża rozbieżność? Powodem jest prawdopodobnie fakt, że metryka \code{EM} dokonuje strukturalnego porównania przewidzianych i wzorowych zapytania, a ten sam cel może być realizowany przez dwa całkowicie różne zapytania SQL. W przypadku wcześniejszych rozwiązań problem ten nie był tak widoczny, ponieważ były one uczone na części treningowej i w związku z tym nauczyły się bardziej preferować pewne sposoby generowania zapytań od innych. \code{C3} produkuje za to zapytania w sposób znacznie bardziej swobodny, co powoduje problemy ze wstrzeleniem się w oczekiwaną odpowiedź, co nie oznacza, że jest ona błędna. 

Słusznie może pojawić się obawa, że otrzymywane z wykorzystaniem \code{C3} wyniki ewaluacji nie są wiarygodne. Przypuszczalnie model językowy \code{GPT-3.5-Turbo} mógł bowiem być trenowany na wykorzystywanych podczas ewaluacji danych lub im podobnych i w związku z tym osiągać w testach zawyżone wyniki. Sytuacja taka nazywana jest wyciekiem danych. Dane treningowe nie mogły zawierać przykładów identycznych, gdyż żadne polskie tłumaczenie zbioru \code{Spider} wcześniej nie istniało. Artykuł \bibtitle{Rethinking Benchmark and Contamination for Language Models with Rephrased Samples} \mycite{Yang2023} wskazuje jednak, że kontaminacja może zostać wprowadzona do zbioru treningowego za pomocą tłumaczeń na inne języki, tak więc angielski zbiór \code{Spider} oraz jego istniejące już tłumaczenia mogą stanowić zagrożenie dla rzetelności testów.

Jeśli chodzi o publiczny ranking zbioru \code{Spider} to takiej obawy nie ma, ponieważ część testowa jest utajona i nie ma możliwości, aby \code{OpenAI} nawet przez przypadek dopuściło do dołączenia tych danych do zbioru treningowego. W niniejszej pracy, jak też wielu innych, z powodu braku dostępności części testowej jej rolę przejmuję publicznie dostępna część pierwotnie nazwana walidacyjną, więc obawa o wyciek danych staje się realna. Zgodnie z artykułem wprowadzającym model \code{GPT-3} \mycite{Brown2020} (wraz z udostępnieniem \code{GPT-3.5} nie został wydany kolejny) \code{OpenAI} dokonuje usuwania ze zbioru treningowego danych pokrywających się z wykorzystywanymi przez nich zbiorami testowymi. Swojego modelu nie testowali na zbiorze \code{Spider}, więc prawdopodobnie specjalne wysiłki nie zostały podjęte w celu wykluczenia tych danych z treningu. Trzeba się więc liczyć z tym, że dane przedstawione w powyższej tabeli mogą być zawyżone.

W przypadku \code{C3} należy zaakcentować dość długi czas potrzebny na wygenerowanie odpowiedzi na dostarczone pytania, ponieważ dla każdego wymagane jest około 40 sekund. Inną zauważoną wadą jest bardzo duża zależność od \code{OpenAI}. Wiąże się to oczywiście z kosztami, ale wprowadza również komplikacje w kwestii utrzymania działania całego rozwiązania, gdyż \code{OpenAI} może dowolnie zmieniać cenniki, usuwać stare modele, czy modyfikować interfejsy.