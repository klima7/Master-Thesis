\section{Model RAT-SQL}
\code{RAT-SQL} to bardzo rozpoznawalny model, który był krzyżowany na przestrzeni czasu z różnymi innymi algorytmami i jego warianty można znaleźć w rankingu zbioru \code{Spider} na wielu pozycjach. Rozwiązanie to zostało zaproponowane w 2020 roku przez Bailin Wang oraz Richarda Shin \mycite{Wang2019}. Doczekało się dwóch kolejnych iteracji, określanych jako \code{RAT-SQL v2} oraz \code{RAT-SQL v3}. W niniejszej pracy rozważana jest ta ostatnia, której kod dostępny jest na platformie \code{GitHub} w repozytorium Microsoftu \mycite{ratsql-repository}. Model ten produkuje zapytania bez wartości, co ogranicza jego praktyczne zastosowanie.

\subsection{Działanie}
Nazwa modelu \code{RAT-SQL} pochodzi od angielskiego wyrażenia \textit{Relation Aware Transformer}, które doskonale opisuje to, co jest w tym rozwiązaniu najważniejsze. Wykorzystuję ono bowiem enkodowanie oparte na grafie, które zostało zrealizowane przy pomocy sieci typu transformer. Standardowo sieci takie potrafią analizować jedynie sekwencje, więc konieczne było zmodyfikowanie ich w taki sposób, aby działały również dla grafów. Dzięki dokonanym zmianom stały się one świadome relacji pomiędzy poszczególnymi elementami sekwencji.

Wszystkie sieci typu transformer posiadają element odpowiedzialny za uczenie się powiązań pomiędzy elementami wejściowymi w postaci mechanizmu uwagi (ang. attention). Autorzy \mbox{\code{RAT-SQL}} zbadali eksperymentalnie jego działanie dla problemu generowania zapytań SQL i zauważyli, że znajdywane połączenia są jednak dość słabe. Postanowili więc zmodyfikować mechanizm uwagi tak, aby poza znajdywaniem powiązań w standardowy, rozmyty sposób, można było jako dodatkowe wejście podawać powiązania z góry znane. 

Relacje, które postanowiono jawnie zakodować poprzez zmodyfikowany mechanizm uwagi to przede wszystkim przynależność poszczególnych kolumn do tabel i powiązania tworzone przez klucze obce. Wykorzystano również etap \code{schema linking} w celu znalezienia połączeń pomiędzy fragmentami pytania i elementami bazy danych. Zrealizowano to poprzez wydobycie z pytania wszystkich n-gramów o długości od 1 do 5 i wyszukane ich dokładnych lub częściowych dopasowań wśród nazw tabel i kolumn. Połączenia typu \code{value link} znaleziono natomiast poprzez rozważenie każdego słowa z pytania osobno i wyszukiwanie go w każdej z dostępnych kolumn. Wszystkie te relacje tworzą skomplikowany graf, który \code{RAT-SQL} pozwala zakodować.

Przed przekazaniem wszystkich informacji wejściowych do transformera dokonującego enkodowania konieczna jest zamiana wartości tekstowych do postaci wektorowej. Uwaga ta tyczy się nazw tabel i kolumn oraz pytania. \code{RAT-SQL} pozwala dokonać takiej konwersji na dwa sposoby: z wykorzystaniem gotowych embeddingów nauczonych metodą \code{GloVe} lub pretrenowanym modelem \code{BERT}. Po enkodowaniu musi nastąpić dekodowanie, w którym wykorzystano rekurencyjne komórki \code{LSTM} do generowania akcji, pozwalających zbudować drzewo \code{AST}. Gdy to nastąpi to drzewo zamieniane jest na tradycyjną postać zapytania SQL.

\subsection{Modyfikacje dla języka polskiego}
Przystosowanie \code{RAT-SQL} do języka polskiego okazało się dość wymagające ze względu na dużą bazę kodu. W pierwszej kolejności koniecznym było testowe uruchomienie tego rozwiązania, co już sprawiło problemy. W repozytorium autorów dostępny jest plik \code{Dockerfile}, teoretycznie umożliwiający proste wystartowanie środowiska, lecz budowa obrazu dockerowego okazała się zwracać błędy ze względu na wygaśnięte klucze \code{GPG}. Po naprawieniu tego problemu niezbędna była zmiana wersji kilku pakietów języka Python ze względu na niezgodności. W ramach tego zmieniono na bardziej aktualną bibliotekę \code{pytorch}, która jest wykorzystywana do tworzenia sieci neuronowych, gdyż starsza wydawała się nie wspierać architektury posiadanej karty graficznej.

Najważniejszą wśród dokonanych modyfikacji była zmiana mechanizmu tworzenia wektorowych reprezentacji tekstów na etapie enkodowania. W oryginalnej implementacji wykorzystany został w tym celu model \code{BERT}, a dokładnie wariant \code{bert-large-uncased-whole-word-masking} z platformy \code{Hugging Face}. Był on trenowany na tekstach angielskich, co go dla wersji polskiej dyskwalifikuje, a ponadto wprowadzana przez niego duża liczba parametrów sprawiała problemy z uruchomieniem na posiadanym sprzęcie. Postanowiono zastąpić go wielojęzycznym odpowiednikiem, trenowanym na 104 językach, oznaczonym na platformie \code{HF} identyfikatorem \code{bert-base-multilingual-uncased}. Posiada on trzykrotnie mniejszą liczbę parametrów, co umożliwia uruchomienie, lecz z pewnością wpływa na niższą skuteczność finalnego rozwiązania. Dostępne są również warianty trenowane tylko na języku polskim, lecz je wykluczono, ponieważ jak wcześniej zauważono, nazwy tabel i kolumn nawet w polskich bazach są najczęściej utrzymywane po angielsku -- całkowicie polski wariant modelu \code{BERT} nie pokryłby tego przypadku. 

Jako alternatywną metodę tworzenia reprezentacji tekstów \code{RAT-SQL} wykorzystuję embeddingi nauczone metodą \code{GloVe}. Nie udało się niestety znaleźć dla nich odpowiednika wielojęzycznego, więc wykorzystano embeddingi całkowicie polskie, zamieszczone w repozytorium \code{polish-nlp-resources} \mycite{polish-nlp-resources}. Dokładnie wybrano wariant \code{300d}, w którym są one wektorami o długości trzystu elementów, ponieważ takiej długości były również oryginalnie zastosowanie embeddingi angielskie.

Wśród innych dokonanych zmian znalazła się modyfikacja listy słów nazywanych \code{stop words}, czyli powszechnie występujących, lecz nie niosących w sobie wiele informacji (i, w, z, na, do, się, o). Słowa takie są pomijane podczas poszukiwania częściowych dopasowań na etapie \code{schema linking} i należało podmienić je na polskie odpowiedniki. Ponadto w rozważanej bazie kodu kilka fragmentów wykorzystuję lematyzację, czyli zamianę słów na ich bazowe formy. W oryginalnej postaci zakłada ona pracę na tekstach angielskich, więc należało dostarczyć polską implementację, do czego wykorzystano bibliotekę \code{Stanza}. Ostatecznie etap \code{schema linking} bazował na znajdywaniu częściowych powtórzeń pomiędzy elementami bazy a fragmentami pytania. Jako że nazwy tabel i kolumn często nie zawierają polskich znaków, a pytania je posiadają, to słusznym działaniem wydało się zmodyfikowanie mechanizmu porównywania tak, aby ignorował różnicę w znakach specjalnych.

\subsection{Eksperymenty}

W ramach eksperymentów postanowiono wytrenować i przetestować model \code{RAT-SQL} w obu wariantach: z enkodowaniem tekstów za pomocą pretrenowanego modelu \code{BERT} oraz z enkodowaniem za pomocą embeddingów \code{GloVe}. W pierwszej konfiguracji model liczy 244 miliony parametrów, a w drugiej zaledwie 17 milionów, z czego wszystkie podlegają optymalizacji. Dla wariantu z \code{BERT} zdecydowano się wytrenować dwie wersje, gdzie jedna zostanie nauczona na polskim zbiorze \code{Pol-Spider}, a druga na zbiorze \code{Mix-Spider}, który zawiera dodatkowo oryginalny zbiór angielski. Niektóre artykuły podają bowiem, że modele trenowane na tego typu połączonych zbiorach tłumaczonych i angielskich osiągają lepsze wyniki niż trenowane jedynie na tłumaczeniach \mycite{Bakshandaeva2022,SpiderPortuguese}, co postanowiono zweryfikować. Po wariancie z \code{GloVe} spodziewano się, że osiągnie słabsze wyniki od \code{BERT}, gdyż zgodnie z artykułem wprowadzającym \code{RAT-SQL} tak właśnie powinno się stać. Jak zostało wcześniej wspomniane, wykorzystane embeddingi \code{GloVe} trenowane były jedynie na tekstach polskich. Z tego powodu ten wariant postanowiono nauczyć dla bardziej ograniczonego scenariusza, w którym nazwy tabel i kolumn zawsze będą w języku polskim i trening przeprowadzono na zbiorze \code{Pol-Spider-PL}.

Autorzy \code{RAT-SQL} oryginalnie trenowali wariant \code{BERT} przez 80 000 kroków, co postanowiono skrócić o połowę. Pomimo tego równoległy trening na obu wspomnianych zbiorach, na dwóch posiadanych kartach graficznych, zajął prawie trzy doby. Wariant \code{GloVe} trenowany był natomiast oryginalnie przez 40 000 kroków, co postanowiono pozostawić bez zmian i w tym przypadku zajęło to niecałe dwie doby. 

\begin{figure}[ht]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[
        width=\linewidth,
        height=\fpeval{0.5*\linewidth},
        xmin=0, xmax=40700,
        grid=major,
        xlabel=krok treningu,
        ylabel={EM Without Values [\%]},
        legend cell align={left},
        legend pos=south east,
      ]
        \addplot table[x=step,y expr=\thisrow{em} * 100,col sep=comma] {plots/ratsql_training_bert.csv}; 
        \addplot table[x=step,y expr=\thisrow{em} * 100,col sep=comma] {plots/ratsql_training_bert_mix.csv}; 
        \addplot table[x=step,y expr=\thisrow{em} * 100,col sep=comma] {plots/ratsql_training_glove.csv}; 
        \legend{BERT + Pol-Spider, BERT + Mix-Spider, GloVe}
      \end{axis}
    \end{tikzpicture}
    \lcaption{Wyniki modeli na zbiorze testowym w trakcie trwania treningu}{Warianty \code{BERT} testowano na zbiorze \code{Pol-Spider}, natomiast \code{GloVe} na zbiorze \code{Pol-Spider-PL}.}
    \label{plot:ratsql-accuracy}
  \end{center}
\end{figure}

Podczas treningu wagi modeli były regularnie zapisywane na dysku w odstępie 1000 kroków. Po zakończeniu każdy taki zestaw wag został przetestowany z wykorzystaniem metryki \code{EM Without Values}, co zajęło sumarycznie kilkanaście kolejnych godzin obliczeń. Warianty z \code{BERT} testowane były na zbiorze \code{Pol-Spider}, natomiast wariant z \code{GloVe} na zbiorze \code{Pol-Spider-PL}. Wykres przedstawiający zmianę skuteczności modeli wraz z kolejnymi krokami treningu przedstawiono na rysunku \ref{plot:ratsql-accuracy}. Ostatecznie z każdego z trzech zestawów wag wybrano do dalszych testów ten, który podczas przedstawionej ewaluacji otrzymał najlepsze rezultaty. 

\subsection{Wyniki}
Trzy wytrenowane modele przetestowano na różnych konfiguracjach zbioru \code{Spider}. Wyniki tej analizy przedstawiono w tabeli \ref{tab:ratsql-results}. Można z niej odczytać, że w każdym przypadku najlepiej nauczył się wariant \code{BERT} trenowany na zbiorze \code{Mix-Spider}, nieco niższą skuteczność osiągnął \code{BERT} uczony na zbiorze \code{Pol-Spider}, natomiast model wykorzystujący embeddingi \code{GloVe} znacznie od tych dwóch odstaję. Z wykorzystaniem najlepszego modelu przeprowadzono dalsze testy, w których dokonano jego ewaluacji na wszystkich zbiorach pokrewnych, z podziałem na poziomy trudności zapytań. Wyniki tych testów przedstawiono w tabeli \ref{tab:ratsql-difficulty}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|r|r|r|r|r|}
        \hline
        \multirow{2}{*}{\thead{\small{Wariant}}} &
        \multirow{2}{*}{\small{\thead{zbiór\\treningowy}}} &
        \multicolumn{4}{c|}{\thead{\small{Zbiór testowy}}} \\
        \cline{3-6}
        \multirow{2}{*}{} &
        \multirow{2}{*}{} &
        \thead{\small{Pol-Spider}} &
        \thead{\small{Pol-Spider-PL}} &
        \thead{\small{Pol-Spider-EN}} &
        \thead{\small{En-Spider}} \\
        \hline
        BERT & Pol-Spider & 53,1 & 56,6 & 49,7 & 42,8 \\
        BERT & Mix-Spider & 58,1 & 61,0 & 55,1 & 68,8 \\
        GloVe & Pol-Spider-PL & 19,3 & 27,5 & 11,2 & \s2,8 \\
        \hline
    \end{tabular}
    \lcaption{Wyniki modeli otrzymane metodą \code{RAT-SQL} na wariantach zbioru \code{Spider}}{Wartości w komórkach tabeli to wyniki metryki \code{EM Without Values} wyrażone procentowo.}
    \label{tab:ratsql-results}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{|l|R{0.10\textwidth}|R{0.10\textwidth}|R{0.10\textwidth}|R{0.10\textwidth}|R{0.10\textwidth}|}
        \hline
        \thead{Zbiór} & \thead{Easy} & \thead{Medium} & \thead{Hard} & \thead{Extra\\Hard} & \thead{Razem} \\
        \hline
        Pol-Spider & 73,8 & 58,7 & 50,9 & 40,4 & 58,1 \\
        Pol-Spidersyn & 58,5 & 45,1  & 41,2  & 26,9  & 44,5 \\
        Pol-Spiderdk & 60,9 & 36,4 & 31,1 & 17,6 & 37,0 \\
        Pol-Sparc & 62,1 & 42,2  & 10,0  & 25,0  & 53,6 \\
        Pol-Cosql & 59,6 & 47.5  & 22,1  & 29,4  & 48,7 \\
        \hline
    \end{tabular}
    \lcaption{Wyniki najlepszego modelu \code{RAT-SQL} na zbiorach pokrewnych}{Najlepszym modelem jest wariant \code{BERT} uczony na zbiorze dwujęzycznym. Wartości w komórkach tabeli to wyniki metryki \code{EM Without Values} wyrażone procentowo.}
    \label{tab:ratsql-difficulty}
\end{table}

\subsection{Analiza}
Istotną obserwacją jest to, że model trenowany na połączeniu zbioru polskiego i angielskiego nauczył się lepiej przewidywać zapytania SQL od tego uczonego wyłącznie na zbiorze polskim. Jest to zgodne z eksperymentami przeprowadzonymi i opisanymi również przez innych badaczy, które podsumowano w tabeli \ref{tab:ratsql-translations-results}. Można z niej odczytać rezultaty, które udało się osiągnąć twórcom dwóch innych tłumaczeń zbioru \code{Spider}, którzy także eksperymentowali z metodą \code{RAT-SQL} i nauką na różnych wariantach swoich zbiorów. W przypadku zbioru portugalskiego zysk otrzymany na poszerzonym zbiorze jest dość niewielki, natomiast dla zbioru rosyjskiego wynosi aż 6 punktów procentowych. Korzyść dla tłumaczenia polskiego jest dość duża i bliższa temu ostatniemu. 

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|r|r|}
        \hline
        \multirow{2}{*}[-0.8em]{\thead{Tłumaczenie \\zbioru Spider}} & \multicolumn{2}{c|}{\thead{Zbiór treningowy}} \\
        \cline{2-3}
        \multirow{2}{*}{} & \thead{Tłumaczenie} & \thead{Tłumaczenie\\ + angielski} \\
        \hline
        \multicolumn{1}{|l|}{Rosyjskie (\code{PAUQ})} & 51,0 & 57,0 \\
        \multicolumn{1}{|l|}{Portugalskie} & 58,8 & 59,5 \\
        \multicolumn{1}{|l|}{Polskie (\code{Pol-Spider})} & 53,1 & 58,1 \\
        \hline
    \end{tabular}
    \lcaption{Wyniki osiągane przez \code{RAT-SQL} dla różnych zbiorów treningowych}{Dane dotyczą modelu w wariancie \code{BERT}. Wartości w komórkach tabeli to wyniki metryki \code{EM Without Values} wyrażone procentowo.}
    \label{tab:ratsql-translations-results}
\end{table}

Wyniki osiągnięte przez model w wariancie z \code{GloVe} okazały się być znacznie niższe w porównaniu z wariantami \code{BERT}. Było to spodziewane, biorąc pod uwagę choćby liczbę parametrów, jednak wydaje się, że wyniki metryk mogłyby być nieco wyższe. Wytrenowany przez twórców \code{RAT-SQL} model odstaję od wariantu \code{BERT} o około 7 punktów procentowych, a w naszym przypadku różnica wynosi ponad 20 punktów. Powodem tego może być przypuszczalnie gorsza jakość embeddingów \code{GloVe} dla języka polskiego niż angielskiego, lecz dalszych eksperymentów zaniechano. Model w tym wariancie mógłby stanowić nieco słabszą alternatywę, którą można szybko wytrenować i uruchamiać na mniej wydajnych urządzeniach. Aktualna dokładność wydaję się jednak za niska, aby to znalazło praktyczne zastosowanie.

Dane liczbowe w tabeli \ref{tab:ratsql-difficulty} pokazują, że wraz ze zwiększaniem się poziomu trudności zapytań maleje skuteczność modelu, co jest spodziewanym zachowaniem. Niewielkie rozbieżności od tej reguły można zaobserwować jedynie dla zbiorów \code{Pol-Cosql} oraz \code{Pol-Sparc}. Przyczyną tego jest zapewne niewielka liczba znajdujących się w nich trudnych zapytań, przez co kilka pozornie wymagających mogło znacząco zachwiać wynikami.

Efekty uzyskane na zbiorze \code{Pol-Spidersyn} okazały się wyraźnie słabsze w porównaniu do tych na \code{Pol-Spider}, bo różnica wynosi aż 13,57 punktów procentowych. Potwierdza to obserwację przedstawioną w artykule wprowadzającym \code{Spider-Syn}. Mówi ona, że modele mają duże problemy z odpowiadaniem na pytania, w których użytkownik nie znając dokładnie schematu bazy danych, posługuję się synonimami.

Niskie wyniki, chociaż również spodziewane, osiągnął model na zbiorze \code{Spider-DK}. Zawiera on bowiem pytania wymagające znajomości wiedzy domenowej, z którą uważa się, że modele sobie często nie radzą. Spadek, który nastąpił w tym przypadku względem zbioru \code{Pol-Spider}, wynosi 21,07 punktów procentowych. W celu dokonania głębszej analizy możliwe jest obliczenie dokładności modelu w obrębie każdego typu wiedzy domenowej z osobna, czego jednak postanowiono nie robić.