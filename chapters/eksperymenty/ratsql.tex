\section{RAT-SQL}
\code{RAT-SQL} to bardzo rozpoznawalny model, który był krzyżowany na przestrzeni czasu z różnymi innymi algorytmami i jego warianty można znaleźć w rankingu zbioru \code{Spider} na wielu pozycjach. Rozwiązanie to zostało zaproponowane w 2021 roku przez Bailin Wang oraz Richarda Shin \cite{Wang2019}. Doczekało się dwóch kolejnych iteracji, określanych jako \code{RAT-SQL v2} oraz \code{RAT-SQL v3}. W niniejszej pracy rozważana jest ta ostatnia, której kod dostępny jest na platformie Github w repozytorium Microsoftu \cite{ratsql-repository}. Model ten produkuje jednak zapytania bez wartości, co ogranicza jego praktyczne zastosowanie.

\subsection{Działanie}
Nazwa modelu \code{RAT-SQL} pochodzi od angielskiego wyrażenia "Relation Aware Transformer", które doskonale opisuje to, co jest w tym rozwiązaniu najważniejsze. Wykorzystuję ono bowiem enkodowanie oparte na grafie, które zostało zrealizowane przy pomocy sieci typu transformer. Standardowo sieci takie potrafią analizować jedynie sekwencje, więc konieczne było zmodyfikowanie ich w ten sposób, aby działały również dla grafów. Dzięki dokonanym zmianom stały się one świadome relacji pomiędzy poszczególnymi elementami sekwencji.

Wszystkie sieci typu transformer posiadają element odpowiedzialny za uczenie się powiązań pomiędzy elementami wejściowymi w postaci mechanizmu uwagi (ang. attention). Autorzy \code{RAT-SQL} zbadali eksperymentalnie działanie tego mechanizmu dla problemu generowania zapytań SQL i zauważyli, że znajdywane połączenia są jednak dość słabe. Postanowili więc zmodyfikować mechanizm uwagi tak, aby poza znajdywaniem powiązań w standardowy, rozmyty sposób można było jako dodatkowe wejście podać powiązania z góry znane. 

Relacje, które postanowiono jawnie zakodować poprzez zmodyfikowany mechanizm uwagi to przede wszystkim przynależność poszczególnych kolumn do tabel i powiązania tworzone przez klucze obce. Wykorzystano również etap \code{schema linking} w celu znalezienia połączeń pomiędzy fragmentami pytania i elementami bazy danych. Zrealizowano to poprzez wydobycie z pytania wszystkich n-gramów o długości od 1 do 5 i wyszukane ich dokładnych lub częściowych dopasowań wśród nazw tabel i kolumn. Połączenia typu \code{value link} znaleziono natomiast poprzez rozważenie każdego słowa z pytania osobno i wyszukiwanie go w każdej z dostępnych kolumn. Wszystkie te relacje tworzą skomplikowany graf, który \code{RAT-SQL} pozwala zakodować.

Przed przekazaniem wszystkich informacji wejściowych do transformera dokonującego enkodowania konieczna jest zamiana wartości tekstowych do postaci wektorowej. Uwaga ta tyczy się nazw tabel i kolumn oraz pytania. \code{RAT-SQL} pozwala dokonać takiej konwersji na dwa sposoby: z wykorzystaniem gotowych embeddingów uzyskanych metodą \code{GloVe} lub pretrenowanym modelem \code{BERT}. Po enkodowaniu musi nastąpić dekodowanie, w którym wykorzystano rekurencyjne komórki \code{LSTM} do generowania akcji pozwalających zbudować drzewo \code{AST}.

\subsection{Modyfikacje dla języka polskiego}
Przystosowanie \code{RAT-SQL} do języka polskiego okazało się dość wymagające ze względu na dużą bazę kodu. W pierwszej kolejności koniecznym było testowe uruchomienie tego rozwiązania, co sprawiło problemy. W repozytorium autorów dostępny jest plik \code{Dockerfile} teoretycznie umożliwiający proste wystartowanie środowiska, lecz budowa obrazu dockerowego okazała się zwracać błędy ze względu na wygaśnięte klucze \code{GPG}. Po naprawieniu tego problemu niezbędna była zmiana wersji kilku pakietów języka Python ze względu na niezgodności. W ramach tego zmieniono na bardziej aktualną bibliotekę \code{pytorch}, która jest wykorzystywana do tworzenia sieci neuronowych, gdyż starsza wydawała się nie wspierać architektury posiadanej karty graficznej.

Najważniejszą wśród dokonanych modyfikacji była ta związana z aktualizacją mechanizmu tworzenia wektorowych reprezentacji tekstów na etapie enkodowania. W oryginalnej implementacji wykorzystany został w tym celu model \code{BERT}, a dokładnie wariant \code{bert-large-uncased-whole-word-masking} z platformy \code{Hugging Face}. Był on trenowany na tekstach angielskich, co go dla wersji polskiej dyskwalifikuje, a ponadto wprowadzana przez niego duża liczba parametrów sprawiała problemy z uruchomieniem na posiadanym sprzęcie. Postanowiono zastąpić go wielojęzycznym odpowiednikiem, trenowanym na 104 językach, oznaczonym na platformie HF nazwą \code{bert-base-multilingual-uncased}. Posiada on trzykrotnie mniejszą liczbę parametrów, co umożliwia uruchomienie, lecz z pewnością wpływa na mniejszą skuteczność finalnego rozwiązania. Dostępne są również warianty trenowane tylko na języku polskim, lecz je wykluczono, ponieważ jak wcześniej zauważono, nazwy tabel i kolumn nawet w polskich bazach są najczęściej utrzymywane po angielsku - całkowicie polski wariant modelu \code{BERT} nie pokryłby tego przypadku. Jako alternatywną metodę tworzenia reprezentacji tekstów \code{RAT-SQL} wykorzystuję embeddingi nauczone metodą \code{GloVe}. Nie udało się niestety znaleźć dla nich odpowiednika wielojęzycznego, więc wykorzystano embeddingi całkowicie polskie, zamieszczone w repozytorium \code{polish-nlp-resources}. Dokładnie wybrano wariant \code{300d} w którym są one wektorami o długości trzystu elementów, ponieważ takiej długości były również oryginalnie zastosowanie embeddingi angielskie.

Wśród innych dokonanych zmian znalazła się modyfikacja listy słów nazywanych \code{stop words}, czyli powszechnie występujących, lecz nie niosących w sobie wiele informacji (i, w, z, na, do, się, o). Słowa takie są pomijane podczas poszukiwania częściowych dopasowań na etapie \code{schema linking} i należało podmienić je na polskie odpowiedniki. Ponadto w rozważanej bazie kodu kilka fragmentów wykorzystuję lematyzację, czyli zamianę słów na ich bazowe formy. W oryginalnej postaci zakłada ona pracę na tekstach angielskich, więc należało dostarczyć polską implementację, do czego wykorzystano bibliotekę \code{stanza}. Ostatecznie etap \code{schema linking} bazował na znajdywaniu częściowych powtórzeń pomiędzy elementami bazy, a fragmentami pytania. Jako, że nazwy tabel i kolumn często nie zawierają polskich znaków, a pytania je posiadają, to słusznym działaniem było zmodyfikowanie mechanizmu porównywania tak, aby ignorował różnicę w znakach specjalnych.

\subsection{Eksperymenty}

\subsubsection{Opis eksperymentów}
W ramach eksperymentów postanowiono wytrenować i przetestować model \code{RAT-SQL} w obu wariantach: z enkodowaniem tekstów za pomocą pretrenowanego modelu \code{BERT} oraz z enkodowaniem za pomocą embeddingów \code{GloVe}. 

Dla wariantu z \code{BERT} zdecydowano się wytrenować dwie wersje, gdzie jedna zostanie nauczona na polskim zbiorze \code{pol-spider}, a druga na zbiorze \code{mix-spider}, który zawiera dodatkowo oryginalny zbiór angielski. Niektóre artykuły podają bowiem, że modele trenowane na tego typu połączonych zbiorach tłumaczonych i angielskich osiągają lepsze wyniki niż te trenowane jedynie na tłumaczeniach, co postanowiono zweryfikować.

Po wariancie modelu z \code{GloVe} spodziewano się, że osiągnie słabsze wyniki od tego drugiego, gdyż w zgodnie z artykułem wprowadzającym \code{RAT-SQL} tak właśnie powinno się stać. Jak zostało wcześniej wspomniane, wykorzystane embeddingi \code{GloVe} trenowane były jedynie na tekstach polskich. Z tego powodu ten wariant postanowiono nauczyć dla bardziej ograniczonego i prostszego scenariusza, w którym nazwy tabel i kolumn zawsze będą w języku polskim i trening przeprowadzono na zbiorze \code{pol-spider-pl}.

\subsubsection{Przebieg treningu}

\begin{figure}[ht]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[
        width=\linewidth,
        height=\fpeval{0.5*\linewidth},
        xmin=0, xmax=40700,
        grid=major,
        xlabel=krok treningu,
        ylabel=EM without values,
        legend cell align={left},
        legend pos=south east,
      ]
        \addplot table[x=step,y=em,col sep=comma] {plots/ratsql_training_bert.csv}; 
        \addplot table[x=step,y=em,col sep=comma] {plots/ratsql_training_bert_mix.csv}; 
        \addplot table[x=step,y=em,col sep=comma] {plots/ratsql_training_glove.csv}; 
        \legend{BERT + pol-spider, BERT + mix-spider, GloVe}
      \end{axis}
    \end{tikzpicture}
    \caption{Wyniki modeli na zbiorze testowym dla kolejnych kroków treningu}
    \label{plot:ratsql-accuracy}
  \end{center}
\end{figure}

Autorzy {RAT-SQL} oryginalnie trenowali wariant \code{BERT} przez 80 000 kroków, co postanowiono skrócić o połowę. Pomimo tego równoległy trening na obu wspomnianych zbiorach, na dwóch kartach graficznych, zajął prawie trzy doby. Wariant \code{GloVe} trenowany był natomiast oryginalnie przez 40 000 kroków, co postanowiono pozostawić bez zmian i w tym przypadku zajęło to niecałe dwie doby. 

Podczas treningu wagi modeli były regularnie zapisywane na dysku w odstępie 1000 kroków. Po zakończeniu treningu każdy taki zestaw wag został przetestowany z wykorzystaniem metryki \code{EM without values}, co zajęło sumarycznie kilkanaście godzin obliczeń. Warianty z \code{BERT} testowane były na części walidacyjnej zbioru \code{pol-spider}, natomiast wariant z \code{GloVe} na zbiorze \code{pol-spider-pl}. Wykres przedstawiający zmianę skuteczności modeli wraz z kolejnymi krokami treningu przedstawiono na rysunku \ref{plot:ratsql-accuracy}. Ostatecznie z każdego z trzech zestawów wag wybrano do dalszych testów ten, który podczas przedstawionej ewaluacji otrzymał najlepsze rezultaty. 

\subsubsection{Wyniki}
Trzy wytrenowane modele przetestowano na różnych konfiguracjach zbioru \code{Spider}. Wyniki tej analizy przedstawiono w tabeli \ref{tab:ratsql-results}. Można z niej odczytać, że w każdym przypadku najlepiej nauczył się wariant \code{BERT} trenowany na zbiorze \code{mix-spider}, nieco niższą skuteczność osiągnął wariant \code{BERT} uczony na zbiorze \code{pol-spider}, natomiast model wykorzystujący embeddingi \code{GloVe} znacznie od tych dwóch odstaję. Z wykorzystaniem najlepszego modelu przeprowadzono dalsze testy w których dokonano jego ewaluacji na wszystkich zbiorach pokrewnych z podziałem na poziomy trudności zapytań. Wyniki tych testów przedstawiono w tabeli \ref{tab:ratsql-difficulty}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|r|r|r|r|r|}
        \hline
        \multirow{2}{*}{\thead{\small{Wariant}}} &
        \multirow{2}{*}{\small{\thead{zbiór\\treningowy}}} &
        \multicolumn{4}{c|}{\thead{\small{Zbiór testowy}}} \\
        \cline{3-6}
        \multirow{2}{*}{} &
        \multirow{2}{*}{} &
        \thead{\small{pol-spider}} &
        \thead{\small{pol-spider-pl}} &
        \thead{\small{pol-spider-en}} &
        \thead{\small{en-spider}} \\
        \hline
        BERT & pol-spider & 53,14 & 56,57 & 49,71 & 49,71 \\
        BERT & mix-spider & 58,07 & 61,02 & 55,12 & 61,31 \\
        GloVe & pol-spider-pl & 19,34 & 27,46 & 11,21 & \s2,80 \\
        \hline
    \end{tabular}
    \caption{Wyniki modeli otrzymane metodą \code{RAT-SQL} na wariantach zbioru \code{Spider}}
    \label{tab:ratsql-results}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{|l|R{0.10\textwidth}|R{0.10\textwidth}|R{0.10\textwidth}|R{0.10\textwidth}|R{0.10\textwidth}|}
        \hline
        \thead{Zbiór} & \thead{Easy} & \thead{Medium} & \thead{Hard} & \thead{Extra} & \thead{Razem} \\
        \hline
        pol-spider & 73,79 & 58,74 & 50,86 & 40,36 & 58,07 \\
        pol-spiderdk & 60,91 & 36,38 & 31,81 & 17,62 & 37,00 \\
        pol-spidersyn & 58,48 & 45,07  & 41,18  & 26,86  & 44,50 \\
        pol-cosql & 59,58 & 47.46  & 22,06  & 29,41  & 48.70 \\
        pol-sparc & 62,08 & 42,23  & 10,00  & 25,00  & 53,56 \\
        \hline
    \end{tabular}
    \caption{Wyniki najlepszego modelu \code{RAT-SQL} na zbiorach pokrewnych z podziałem na poziomy trudności zapytań SQL}
    \label{tab:ratsql-difficulty}
\end{table}

\subsubsection{Analiza}
Istotną obserwacją jest to, że model trenowany na połączeniu zbioru polskiego i angielskiego nauczył się lepiej przewidywać zapytania SQL od modelu uczonego wyłącznie na zbiorze polskim. Jest to zgodne z eksperymentami przeprowadzonymi i opisanymi również przez innych badaczy, które podsumowano w tabeli \ref{tab:ratsql-translations-results}. Można z niej odczytać rezultaty, które udało się osiągnąć twórcom dwóch innych tłumaczeń zbioru \code{Spider}, którzy także eksperymentowali z metodą \code{RAT-SQL} i nauką na różnych zbiorach. W przypadku zbioru portugalskiego zysk uzyskany na poszerzonym zbiorze jest dość niewielki, natomiast dla zbioru Rosyjskiego wynosi aż 6 punktów procentowych - przyczyny tej rozbieżności są dość niejasne. Korzyść uzyskana w przypadku tłumaczenia polskiego jest bliższa temu ostatniemu. 

% Bazując na tej tabeli można również wyciągnąć wniosek, że wprowadzone modyfikacje wariantu \code{BERT} dla języka polskiego są jak najbardziej poprawne, ponieważ osiągnięte wyniki w żadnym stopniu nie odstają od tych osiągniętych na innych tłumaczeniach.

\begin{table}[ht!]
    \centering
    \begin{tabular}{|l|r|r|}
        \hline
        \multirow{2}{*}{\thead{Tłumaczenie\\zbioru Spider}} & \multicolumn{2}{c|}{\thead{Zbiór treningowy}} \\
        \cline{2-3}
        \multirow{2}{*}{} & \thead{Tłumaczenie} & \thead{Tłumaczenie + angielski} \\
        \hline
        Rosyjskie (\code{PAUQ}) & 51,0 & 57,0 \\
        Portugalskie & 58,8 & 59,5 \\
        Polskie (\code{Pol-Spider}) & 53,1 & 58,1 \\
        \hline
    \end{tabular}
    \caption{Zestawienie wyników osiągniętych przez wariant \code{BERT} modelu \code{RAT-SQL} dla różnych tłumaczeń zbioru \code{Spider} i zbiorów treningowych}
    \label{tab:ratsql-translations-results}
\end{table}

Dane liczbowe w tabeli \ref{tab:ratsql-difficulty} pokazują, że wraz ze zwiększaniem się poziomu trudności zapytań maleje skuteczność modelu, co jest spodziewanym zachowaniem. Niewielkie rozbieżności od tej reguły można zaobserwować jedynie dla zbiorów \code{pol-cosql} oraz \code{pol-sparc}. Przyczyną tego jest zapewne niewielka liczba znajdujących się w nich trudnych zapytań, przez co kilka pozornie wymagających mogło znacząco zachwiać wynikami.

W dalszym ciągu odwołując się do tabeli \ref{tab:ratsql-difficulty} trzeba zauważyć, że wyniki osiągane na wszystkich zbiorach pokrewnych są niższe od uzyskanych na zbiorze \code{pol-spider}. Jedną z przyczyn tego jest fakt, że finalne wagi modelu wybrano tak, aby maksymalizowały wyniki właśnie dla zbioru \code{pol-spider}. Dla pozostałych zbiorów model też powinien osiągać wysokie wyniki, lecz nie tak optymalne.

Efekty uzyskane na zbiorze \code{pol-spidersyn} okazały się wyraźnie słabsze w porównaniu do tych na \code{pol-spider}, bo różnica wynosi aż 13,57 punktów procentowych. Potwierdza to obserwację przedstawioną w artykule wprowadzającym \code{Spider-Syn}. Mówi ona, że modele mają duże problemy z odpowiadaniem na pytania w których użytkownik nie znając dokładnie schematu bazy danych posługuję się synonimami.

Niskie wyniki, chociaż spodziewane, osiągnął model również na zbiorze \code{Spider-DK}. Zawiera on bowiem pytania wymagające znajomości wiedzy domenowej, z którą uważa się, że modele sobie często nie radzą. Spadek, który nastąpił w tym przypadku względem zbioru \code{pol-spider} wynosi 21,07 punktów procentowych.