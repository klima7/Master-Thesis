\chapter{Przegląd podejść do problemu Text-to-SQL}
Celem niniejszego rozdziału jest przedstawienie zwięzłego przeglądu podejść do problemu \code{Text-to-SQL}. Najpierw wskazane zostaną popularne metryki, wykorzystywane do pomiaru skuteczności tworzonych algorytmów, a następnie naszkicowane wysokopoziomowe podejścia do tego problemu, które ukształtowały się na przestrzeni lat. Zawarta wiedza stanowi podstawę do zrozumienia kolejnej części, w ramach której analizowane i testowane będą różne modele uczenia maszynowego. W celu głębszego zapoznania się z poruszanymi kwestiami można sięgnąć do jednego z wielu artykułów dokonujących przeglądu problemu tłumaczenia zapytań \mycite{Katsogiannis2021,Quamar2022NaturalInterfacesBook,SurveyDL}.

\section{Stosowane metryki}
\label{section:metrics}
Posiadając docelowe zapytanie, które w literaturze anglojęzycznej określane jest mianem \code{gold query}, oraz zapytanie wyprodukowane przez dowolny algorytm, zachodzi potrzeba dokonania oceny tego, w jakim stopniu one sobie odpowiadają. Pozwala to bowiem w liczbowy sposób wyrazić dokładność algorytmu i dokonać jego porównań z innymi. Manualna ocena zwykle nie wchodzi w grę ze względu na swoją czasochłonność, więc opracowanych zostało kilka metryk, które dokonują tego w sposób automatyczny. Te najważniejsze opisano poniżej.

\subsection{Exact Set Match (EM)}
Problemem zwyczajnego porównywania ze sobą docelowych i przewidywanych zapytań SQL jako wartości tekstowych jest to, że mogą się różnić w kolejności elementów niemających znaczenia. Przykładowo zwykle nieistotne jest, w jakiej kolejności zostaną zwrócone poszczególne kolumny. Podobnie nie ma znaczenia uporządkowanie wyrażeń boolowskich połączonych operatorami \code{AND}, czy \code{OR}. Ostatecznie obojętna jest nawet kolejność występowania klauzuli \code{SELECT}, \code{FROM}, czy \code{WHERE} w strukturze zapytania. Metryka \code{Exact Set Match}, nazywana skrótowo \code{EM}, rozwiązuje naszkicowany problem poprzez odpowiednie przetworzenie zapytań i porównywanie poszczególnych składników, traktując je jak zbiory, czyli ignorując kolejność, która jest nieistotna. Należy jednak zauważyć, że w języku SQL można osiągnąć ten sam cel przy pomocy kompletnie różnych instrukcji, zupełnie odmiennych strukturalnie. Rozważana metryka nie jest w stanie poprawnie rozpoznać takich przypadków i w rzeczywistości prawidłowe zapytanie oceni jako błędne.

\subsection{Exact Set Match Without Values (EM Without Values)}
Okazuje się, że szczególnie trudnym do przewidywania fragmentem zapytań SQL są wartości, czyli występujące w nich łańcuchy tekstowe i liczby. Powstające kiedyś modele bardzo słabo radziły sobie z nimi, a często w ogóle ich nie przewidywały -- produkowały zapytania, w których wartości były zamaskowane. Sprawiało to, że zwracane instrukcje nie były do końca prawidłowe, czy kompletne, lecz jeżeli reszta komponentów była odpowiednia, to chciano całe zapytanie zaklasyfikować jako poprawne. W tym celu opracowana została metryka \code{Exact Set Match Without Values}. Jej mechanizm działania jest niemal identyczny jak \code{Exact Set Match}, bo jedyną różnicą jest ignorowanie rozbieżności w wartościach. Przykłady kilku poprawnych i niepoprawnych zapytań dla metryki \code{EM Without Values} przedstawiono na rysunku \ref{fig:em-without-values}.

\begin{figure}[ht!]
  \centering
  \includesvg[width=0.85\linewidth]{images/em_without_values}
  \caption{Prawidłowe i nieprawidłowe zapytania dla metryki \code{EM Without Values}}
  \label{fig:em-without-values}
\end{figure}

\subsection{Execution Accuracy (EX)}
\label{text:execution_accuracy}
Metryka \code{Execution Accuracy}, nazywana skrótowo \code{EX}, wychodzi z prostego założenia, że jeżeli przewidziane zapytanie jest poprawne, to jego wykonanie musi zwrócić takie same rekordy jak zapytanie referencyjne. Ma ona jednak pewne wymagania. Przede wszystkim porównywane zapytania muszą być wykonywalne, więc zawierać wartości. Drugim warunkiem jest posiadanie wypełnionych baz danych, ponieważ w przeciwnym wypadku wszystkie zapytania będą zwracały pustą listę rekordów i tym samym zostawały błędnie uznane za poprawne. Posiadając nawet duże bazy danych, mogą zdarzyć się przypadki, iż takie same rekordy zostaną przypadkowo zwrócone dla dwóch nierównoważnych zapytań, co skończy się błędną klasyfikacją. Stanowi to podstawowe ograniczenie metryki \code{EX}.

\section{Kluczowe cechy modeli}
Dość wyraźny jest podział istniejących algorytmów rozwiązujących problem \code{Text-to-SQL} ze względu na dwie cechy. Pierwsza z nich to kwestia obecności wartości w przewidywanych zapytaniach. Druga natomiast to posiadanie umiejętności wykorzystania zawartości baz danych. Informacje te w oficjalnym rankingu zbioru \code{Spider} są wyszczególnione dla każdego modelu, ponieważ mają dość istotne implikacje, które zostaną opisane w poniższych sekcjach.

\subsection{Przewidywanie wartości}
Algorytmy tłumaczące zapytania naturalne na język SQL dzielą się na dwie podstawowe kategorie: przewidujące kompletne zapytania oraz zapytania bez wartości. Wówczas produkują instrukcje zawierające w ich miejscuspecjalne symbole zastępcze, takie jak \code{value}, czy \code{terminal}. Przykłady wygenerowanych zapytań zgodnie z oboma podejściami przedstawiono na rysunku \ref{fig:query-values}.

\begin{figure}[ht!]
  \centering
  \includesvg[width=1.0\linewidth]{images/query_values}
  \caption{Przykład wygenerowanych zapytań SQL z wartościami i bez}
  \label{fig:query-values}
\end{figure}

Przyczyną wyklarowania się takiego podziału jest fakt, iż wartości stanowią jeden z najtrudniejszych do wygenerowania fragmentów zapytań, a pierwsze z powstających modeli posiadały bardzo niską skuteczność. Słabo generowały wartości lub nie robiły tego wcale. Postanowiono więc obniżyć poziom postawionego sobie zadania poprzez zignorowanie wartości i przewidywanie samej struktury zapytań. Obecnie algorytmy posunęły się jednak znacząco do przodu i osiągają wysokie wyniki nawet w generowaniu kompletnych instrukcji SQL, więc uwaga badaczy przesunęła się na ten bardziej praktyczny przypadek.

\subsection{Wykorzystanie zawartości baz danych}
Drugą istotną cechą, na podstawie której można scharakteryzować wszystkie algorytmy tłumaczenia zapytań naturalnych na SQL, jest to, czy korzystają z zawartości baz danych. Podstawowym elementem, na którym operują, są bowiem schematy. Zawartość natomiast, pomimo że niejednokrotnie zawiera istotne dla generowanego zapytania informacje, to wymaga zwiększenia poziomu komplikacji algorytmu. Wiąże się to także z problemem wydajnego odpytywania nieraz ogromnych zbiorów. Mimo wszystko duża część istniejących modeli w jakiś sposób z zawartości baz danych korzysta.

\section{Wysokopoziomowe podejścia}
Na obecną chwilę w rankingu zbioru \code{Spider} znajdują się modele reprezentujące dwa wysokopoziomowe, całkowicie różne od siebie podejścia. Jedno z nich polega na projektowaniu i trenowaniu modeli dedykowanych, a drugie na wykorzystaniu dużych, pretrenowanych modeli językowych, gdzie żaden trening nie jest potrzebny. 

\subsection{Modele dedykowane}
Przez modele dedykowane rozumie się takie, które zostały stworzone w celu rozwiązania tylko jednego, konkretnego problemu. Przykładem jest tłumaczenie pytań z języka naturalnego na SQL. Takie ograniczenie pozwala na uwzględnienie i uwypuklenie w architekturze modelu oraz procesie treningu wiele istotnej dla danej domeny wiedzy. Z tego powodu możliwe jest zmniejszenie liczby potrzebnych danych i skrócenie czasu treningu. Modele tego typu dla problemu \code{Text-to-SQL} istnieją od bardzo dawna i z biegiem czasu stają się coraz lepsze. W dalszej części rozdziału, w sekcji \ref{specific-stages}, zostało dokładniej opisane ich działanie. Jedynie w ostatnim czasie pojawiła się alternatywa w postaci dużych pretrenowanych modeli językowych.

\subsection{Duże pretrenowane modele językowe}
Duże pretrenowane modele językowe zostały zaadaptowane w bardzo wielu problemach. Są to sieci neuronowe wytrenowane na ogromnych zbiorach danych w celu zyskania wszechstronnej wiedzy. Zapoczątkowało je laboratorium badawcze \code{OpenAI}, które w roku 2018 opracowało pierwszy model językowy ogólnego przeznaczenia o nazwie \code{GPT-1} \mycite{Openai2018}. Szczególną uwagę całego świata zwrócił na siebie jednak dopiero model \code{GPT-3.5}, który stanowi kolejną iterację wcześniej wspomnianego. Najświeższym tego typu modelem i jednocześnie uważanym za najlepszy jest natomiast \code{GPT-4-turbo}, do którego badacze mogą uzyskać dostęp za pomocą przygotowanego API i zintegrować go ze swoimi rozwiązaniami.

Wykorzystanie dużych modeli językowych opiera się na traktowaniu ich jak czarnych skrzynek, które przyjmują na wejście instrukcję w języku naturalnym i zwracają odpowiedź. Te instrukcje wejściowe mają kluczowe znaczenie i określane są mianem promptów. Nawet niewielkie ich modyfikacji mogą drastycznie wpływać na otrzymywane odpowiedzi. W związku z tym powstała cała dyscyplina określana jako \code{prompt engineering}, która zajmuje się tworzeniem oraz optymalizacją promptów w takim kierunku, aby model zwracał oczekiwane odpowiedzi.

W ostatnim czasie rozwiązania bazujące na dużych, pretrenowanych modelach językowych zdominowały górną część rankingu zbioru \code{Spider}. Ich wadą jest jednak to, że wykorzystują płatne modele udostępniane przez \code{OpenAI}, co czyni je zbyt kosztownymi dla wielu zastosowań.

\section{Etapy tłumaczenia zapytań} \label{specific-stages}
Celem niniejszej części jest przedstawienie powszechnie występujących w dedykowanych modelach \code{Text-to-SQL} etapów przetwarzania. Pozwoli to znacznie lepiej zrozumieć sposób działania rozwiązań wybranych do eksperymentów w kolejnym etapie.

Okazuje się, że niemal wszystkie powstałe do tej pory modele opierają się na architekturze \code{enkoder-dekoder}. Oznacza to, że występuje w nich etap enkodowania, podczas którego informacje wejściowe konwertowane są do zwięzłej reprezentacji pośredniej, a następnie w etapie dekodowania na jej podstawie generowana jest ostateczna odpowiedź. Etapy te mogą być zrealizowane na bardzo wiele sposobów. Zwykle wyróżnia się także dodatkową fazę określaną mianem \code{schema linking}, która zwraca informację pozwalającą lepiej dokonać procesu enkodowania. 

\subsection{Schema Linking}
Celem etapu \code{schema linking} jest znalezienie powiązań pomiędzy frazami występującymi w pytaniu a elementami bazy danych, takimi jak tabele, kolumny i wartości. Jest to działanie, które jest intuicyjnie wykonywane nawet przez ludzkich ekspertów, kiedy stoją przed zadaniem skonstruowania nowego zapytania SQL. Przykładowy rezultat takiego działania przedstawiono na rysunku \ref{fig:schema-linking}. W odróżnieniu od dwóch kolejnych etapów ten jest opcjonalny, lecz w praktyce często wykorzystywany.

\begin{figure}[ht!]
  \centering
  \includesvg[width=1.0\linewidth]{images/schema_linking}
  \caption{Przykład wykonania \code{schema linking}}
  \label{fig:schema-linking}
\end{figure}

\code{Schema linking} można rozbić na dwie fazy. Pierwsza polega na odnalezieniu wyrażeń w pytaniach oraz elementów w bazie danych, które mogą być w kontekście budowanego zapytania istotne. Nazywane są one odpowiednio \code{query candidates} oraz \code{database candidates}. Druga faza  wymaga połączenia ze sobą tych elementów i w ten sposób skonstruowania połączeń nazywanych \code{table link}, \code{column link} lub \code{value link}, odpowiednio dla wystąpienia połączenia z tabelą, kolumną lub wartością.


Najprostsza metoda znajdywania kandydatów w pytaniu polega na uwzględnieniu każdego słowa z osobna. Wyrażenia stanowiące odwołania mogą składać się jednak z kilku słów, więc lepszym wyborem wydaje się rozważenie różnych n-gramów, czyli n-elementowych podciągów. Poza tym możliwe jest wykorzystanie analizy \code{NER} w celu znalezienia w pytaniach nazw własnych, które częstą okazują się tworzyć połączenia typu \code{value link}.

Oczywistymi kandydatami, jeśli chodzi o bazę danych, są wszystkie nazwy tabel i kolumn. Znacznie trudniej jest wybrać konkretne wartości, mogące tworzyć \code{value link}, bo z uwagi na ich wielość nie można brać wszystkich pod uwagę. Najczęściej dokonuje się jedynie poszukiwania w bazie konkretnych, podejrzanych wartości, które zostały ujawnione w pytaniu poprzez analizę \code{NER}, czy też zostały zapisane tam w cudzysłowie.

Po znalezieniu kandydatów w pytaniu i bazie danych należy spróbować dokonać ich dopasowania. Problem stanowi to, że pytania naturalne nie odwołują się najczęściej do elementów bazy poprzez dokładnie to samo określenie, lecz używają innych form i synonimów. Sprawia to, że dopasowywanie poprzez szukanie idealnych powtórzeń pomiędzy kandydatami często się nie sprawdza, lepszą metodą jest szukanie częściowych powtórzeń. Możliwe jest również wykorzystanie odległości edycyjnej, czy odległości pomiędzy wektorowymi reprezentacjami wyrażeń.

\subsection{Enkodowanie}
Enkodowanie to etap, którego celem jest konwersja wszystkich danych wejściowych do zwartej, wektorowej postaci, czego dokonuje się za pomocą sieci neuronowej. Minimalnym zestawem informacji wejściowych jest pytanie w języku naturalnym oraz nazwy tabel i kolumn. Warte wzięcia pod uwagę są jednak także przynależności poszczególnych kolumn do tabel oraz relacje tworzone przez klucze podstawowe i obce. Jeżeli wykorzystywany został opisany wcześniej etap \code{schema linking}, to znalezione podczas niego relacje również powinny w tym momencie zostać zakodowane. Poniżej opisane zostały trzy popularniejsze drogi realizacji enkodowania, lecz jest ich znacznie więcej.

Najstarsza metoda enkodowania polega na zakodowaniu pytania naturalnego oraz nazw tabel i kolumn niezależnie od siebie, a następnie połączeniu tych informacji ze sobą na którymś etapie sieci neuronowej, która jest potem trenowana. Przykładami działających tak systemów jest \code{Seq2sql} \mycite{zhong2017seq2sql} oraz \code{Sqlnet} \mycite{xu2017sqlnet}. Wspomniane zakodowanie, czyli konwersja wartości tekstowych do reprezentujących je wektorów, nazywanych \code{embeddingami} (zanurzeniami), odbywa się w najprostszym przypadku poprzez wykorzystanie gotowych i publicznie dostępnych mapowań otrzymanymi algorytmami \code{Word2Vec} \mycite{church2017word2vec}, czy też \code{GloVe} \mycite{Pennington2014}.

Metodą pozwalającą osiągnąć lepsze wyniki od wcześniej wspomnianej jest serializacja informacji wejściowych do jednego długiego tekstu, który następnie jest enkodowany za pomocą pretrenowanego modelu językowego typu enkoder. Jest to zwykle model oparty na architekturze transformerów \mycite{NIPS2017_3f5ee243}, gdzie \code{BERT} \mycite{Devlin2019} jest najczęstszym wyborem. Ich zaletą jest to, że poszczególne słowa enkodują w sposób kontekstowy, czyli na sposób kodowania każdego słowa mają wpływ wszystkie inne. Najbardziej problematyczne w tym podejściu jest to, że nie wszystkie informacje wejściowe da się łatwo zawrzeć w czystym tekście, na przykład relacje uzyskane w wyniku \code{schema linking}, czy klucze obce. W opisany sposób działa \code{IRNet} \mycite{Guo2019}, czy analizowany w kolejnym rozdziale \code{BRIDGE} \mycite{Lin2020}.

Najbardziej elastyczna metoda, pozwalająca na wygodne zakodowanie wszystkich informacji wejściowych, opiera się na wykorzystaniu reprezentacji grafowej. Wówczas poszczególne tabele, kolumny oraz słowa z pytania interpretowane są jako węzły, a znane relacje pomiędzy nimi, w tym te wydobyte na etapie \code{schema linking}, tworzą krawędzie. Czynnikiem najbardziej hamującym powszechne wykorzystanie tej strategii jest trudność związana z przetwarzaniem grafów przez sieci neuronowe. Tą technikę enkodowania wykorzystuje testowany w kolejnej części model \code{RAT-SQL} \mycite{Wang2019} oraz niedawno powstały \code{Graphix-T5} \mycite{graphix-t5}. 

\subsection{Dekodowanie}
Dekodowanie to finalny etap, którego celem jest zbudowanie gotowego zapytania SQL. Bazuje na uzyskanej w fazie enkodowania reprezentacji pośredniej i również jest realizowany z wykorzystaniem sieci neuronowych. Można wskazać trzy główne nurty, w które wpisują się tworzone rozwiązania i zostaną one pokrótce opisane.

Najbardziej ograniczoną strategią jest dekodowanie oparte na szkicach (ang. sketch-based). Zakłada ono przyjęcie pewnego szablonu zapytania SQL z lukami, które należy uzupełnić. W dużej mierze sprowadza to ten etap do zadania klasyfikacji, gdyż luki wymagają zwykle wyboru jednej z dostępnych tabel lub kolumn.  Sprawdza się to jednak wyłącznie w uproszczonych scenariuszach, na przykład zakładających wykorzystanie tylko jednej tabeli, bez żadnych połączeń. Podejście to nie ma więc wielu praktycznych zastosowań i wydaje się być wykorzystywane głównie do poprawiania wyników osiąganych na prostych benchmarkach, jak zbiór \code{WikiSQL}. W ten sposób działające rozwiązania to \code{SqlNet} \mycite{xu2017sqlnet}, \code{TypeSQL} \mycite{yu-etal-2018-typesql}, czy nowszy model zaproponowany przez Jianqiang Ma \mycite{ma-etal-2020-mention}.

Bardziej popularną obecnie metodą dekodowania jest generowanie zapytania od zera, słowo po słowie. Dokonywane jest to poprzez uwzględnienie w konstruowanej sieci elementów rekurencyjnych, takich jak komórki \code{LSTM} \mycite{lstm-review-2019}. Alternatywą jest douczenie jednego z pretrenowanych modeli językowych typu \code{enkoder-dekoder}. W rankingu zbioru \code{Spider} szczególnie popularny jest model \code{T5} \mycite{Raffel2019}. Generowanie zapytań jako zwykłego tekstu ma ten problem, że mogą być one niepoprawne nawet pod względem strukturalnym, więc powstało wiele technik, by temu przeciwdziałać. Dekodowanie to wykorzystywane jest przez \code{Seq2sql} \mycite{zhong2017seq2sql}, \code{BRIDGE} \mycite{Lin2020}, czy \code{RESDSQL} \mycite{Li2023resdsql}.

Za jedne z najlepszych uważane są dekodery bazujące na gramatyce. Podobnie do poprzednich, generują zapytania od zera, element po elemencie, lecz z tą różnicą, że nie są to słowa. Na każdym kroku dekodowania produkują bowiem akcję, która służy do budowania drzewa \code{AST} reprezentującego zapytanie. Ma ono tą zaletę, że w każdym momencie jest poprawne, a po jego ukończeniu można przejść do zapytania w standardowej postaci. Dekodowanie oparte na gramatyce znalazło swoje miejsce w rozwiązaniach \code{IRNet} \mycite{Guo2019}, czy też \code{RAT-SQL} \mycite{Wang2019}.